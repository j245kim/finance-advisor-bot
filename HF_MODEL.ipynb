{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKENS = os.getenv(\"HF_TOKENS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionOutputMessage(role='assistant', content='The capital of France is Paris.', tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "api_key = ''\n",
    "client = InferenceClient(api_key=HF_TOKENS)\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is the capital of France?\"\n",
    "    }\n",
    "]\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    messages=messages,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bye!\n"
     ]
    }
   ],
   "source": [
    "def chatbot_conversation():\n",
    "    messages = [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in ['exit', 'quit', 'bye']:\n",
    "            print(\"Bye!\")\n",
    "            break\n",
    "        \n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "        \n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "            messages=messages,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        bot_reply = completion.choices[0].message['content']\n",
    "        print(f\"Bot: {bot_reply}\")\n",
    "        \n",
    "        messages.append({\"role\": \"assistant\", \"content\": bot_reply})\n",
    "\n",
    "chatbot_conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bye!\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Hugging Face API 키 입력\n",
    "api_key = HF_TOKENS\n",
    "client = InferenceClient(api_key=api_key)\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are an analyst who answers questions accurately based on coin data and newspaper articles, English question/Korean question.\n",
    "\n",
    "#Previous Chat History:\n",
    "{chat_history}\n",
    "\n",
    "#Question: \n",
    "{question} \n",
    "\n",
    "#Context: \n",
    "{context} \n",
    "\n",
    "#Answer:\"\"\"\n",
    ")\n",
    "\n",
    "def chatbot_conversation():\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in ['exit', 'quit', 'bye']:\n",
    "            print(\"Bye!\")\n",
    "            break\n",
    "        \n",
    "        chat_history = \"\\n\".join([msg['content'] for msg in messages])\n",
    "        context = \"Context information here\"  \n",
    "        \n",
    "        prompt_input = prompt.format(chat_history=chat_history, question=user_input, context=context)\n",
    "        \n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt_input}],\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        bot_reply = completion.choices[0].message['content']\n",
    "        print(f\"Bot: {bot_reply}\")\n",
    "        \n",
    "        messages.append({\"role\": \"assistant\", \"content\": bot_reply})\n",
    "\n",
    "chatbot_conversation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문에 대한 답은 비트코인, 도지코인 입니다.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTableQuestionAnswering, pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dsba-lab/koreapas-finetuned-korwikitq\")\n",
    "model = AutoModelForTableQuestionAnswering.from_pretrained(\"dsba-lab/koreapas-finetuned-korwikitq\")\n",
    "\n",
    "# Initialize the question answering pipeline\n",
    "qa_model = pipeline('table-question-answering', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Sample DataFrame for coin price data (no error-related columns)\n",
    "df = pd.DataFrame({\n",
    "    '코인': ['비트코인', '도지코인', '용희코인'],\n",
    "    '가격': ['1억', '5000원', '0원']\n",
    "})\n",
    "\n",
    "def make_answer(q):\n",
    "    if df.empty:\n",
    "        return '데이터가 존재하지 않습니다.'\n",
    "    \n",
    "    answer = qa_model(query=q, table=df)\n",
    "\n",
    "    if not answer['answer']:\n",
    "        return '질문에 대한 답을 찾지 못했습니다.'\n",
    "    \n",
    "    if answer['coordinates'][0][-1] == 0:\n",
    "        answer2 = answer['answer']\n",
    "        return f\"질문에 대한 답은 {answer2} 입니다.\"\n",
    "    else:\n",
    "        answer2 = answer['cells'][0]\n",
    "        return f\"그 코인의 가격은 {answer2} 입니다.\"\n",
    "\n",
    "print(make_answer(\"adfe fse s efes se가격.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
