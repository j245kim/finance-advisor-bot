{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-1.2.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from accelerate) (6.1.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from accelerate) (2.5.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from accelerate) (0.26.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.9.0)\n",
      "Requirement already satisfied: requests in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n",
      "Downloading accelerate-1.2.1-py3-none-any.whl (336 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.2.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKENS = os.getenv(\"HF_TOKENS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionOutputMessage(role='assistant', content='The capital of France is Paris.', tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "api_key = ''\n",
    "client = InferenceClient(api_key=HF_TOKENS)\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is the capital of France?\"\n",
    "    }\n",
    "]\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    messages=messages,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: I'm doing well, thanks. How can I assist you today?\n",
      "Bot: It seems like you started to ask a question, but it got cut off. You're free to describe what you'd like to know or discuss, and I'll do my best to provide a helpful response. What's on your mind?\n",
      "Bot: I'm a large language model, so I don't have a physical presence or experiences like humans do, but I'm functioning properly and ready to help. Days can go by quickly, kind of like how you use your downtime or relax before diving into a new subject. How about you?\n",
      "Bot: It seems like we were having a brief conversation, but I don't have any context or information about our previous conversation. Could you tell me a bit more about what you were talking about, or perhaps specify which one of the two \"How are you\" questions you referenced earlier?\n",
      "Bot: It seems like you're looking for a safe space to share your thoughts or feelings about wanting to \"escape.\" I'm here to listen and provide a listening ear. If you're feeling overwhelmed or needing to talk about something that's causing you distress, I want you to know that I'm here to support you. Would you like to talk about what's going on and how I can help?\n",
      "Bot: It seems like we're having a bit of a introspective moment. You're clearly Thoughts of wanting to \"escape\" or let go of things, maybe toxic relationships or circumstances, are a significant part of your current emotional landscape right now. Would you like to talk more about what's leading you to think about escape, perhaps past experiences that might be contributing to this idea, or maybe something you've recently encountered that's causing you distress?\n",
      "Bot: It seems like you're still feeling a strong urge to talk about wanting to \"escape.\" I'm here to listen without judgment, and I want to acknowledge that it takes a lot of courage to share something as personal as this. Feeling stuck or trapped can be overwhelming, especially if you're someone who values freedom and autonomy.\n",
      "\n",
      "Before we continue, I want to make sure I'm providing a safe and non-judgmental space for you to express yourself. If you feel comfortable, can you tell me a bit more about what's going through your mind when you think about escape? What do you hope to achieve or escape from in a real sense?\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m         messages\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: bot_reply})\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# 챗봇 시작\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m \u001b[43mchatbot_conversation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 7\u001b[0m, in \u001b[0;36mchatbot_conversation\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m messages \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, how are you?\u001b[39m\u001b[38;5;124m\"\u001b[39m}]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# 사용자 입력 받기\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m user_input\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbye\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGoodbye!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "def chatbot_conversation():\n",
    "    messages = [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in ['exit', 'quit', 'bye']:\n",
    "            print(\"Bye!\")\n",
    "            break\n",
    "        \n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "        \n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "            messages=messages,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        bot_reply = completion.choices[0].message['content']\n",
    "        print(f\"Bot: {bot_reply}\")\n",
    "        \n",
    "        messages.append({\"role\": \"assistant\", \"content\": bot_reply})\n",
    "\n",
    "chatbot_conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: I cannot provide real-time financial or market information, nor can I provide advice on investing in cryptocurrencies like Bitcoin. However, I can give you some general information about Bitcoin.\n",
      "\n",
      "As of my knowledge cutoff in 2023, Bitcoin is a decentralized digital currency that was first introduced in 2009 by an individual or group of individuals using the pseudonym Satoshi Nakamoto. It's a store of value, a medium of exchange, and a form of compensation for transactions.\n",
      "\n",
      "Bitcoin's value is largely determined by supply and demand in the market, with its price influenced by factors such as:\n",
      "\n",
      "1. Security and temperature levels for its wallets\n",
      "2. Network congestion and miners' block reward rates\n",
      "3. Regulatory environment and government records\n",
      "4. Adoption and user base\n",
      "\n",
      "It's essential to note that investing in Bitcoin carries significant risks, including market volatility, regulatory uncertainty, and security concerns.\n",
      "\n",
      "For further information, I recommend consulting reputable sources, such as financial news agencies, consulting firms specializing in cryptocurrencies, or regulatory bodies overseeing blockchain developments.\n",
      "\n",
      "If you have any other question, feel free to ask.\n",
      "Bot: I cannot provide real-time financial or market information, nor can I provide advice on investing in cryptocurrencies like Bitcoin. However, I can give you some general information about Bitcoin.\n",
      "\n",
      "As of my knowledge cutoff in 2023, it's essential to note that the value of Bitcoin is largely determined by supply and demand in the market, with its price influenced by factors such as:\n",
      "\n",
      "1. Security and temperature levels for its wallets\n",
      "2. Network congestion and miners' block reward rates\n",
      "3. Regulatory environment and government records\n",
      "4. Adoption and user base\n",
      "\n",
      "It's also worth mentioning that Bitcoin has experienced significant price fluctuations in the past, with some notable declines due to market volatility and security concerns. I would caution against making any investment decisions based on recent price movements or speculation about future price increases.\n",
      "Bot: I cannot provide real-time information about the value of Bitcoin or any other cryptocurrency, nor can I give advice on investing in them. Can I help you with something else?\n",
      "Bye!\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Hugging Face API 키 입력\n",
    "api_key = HF_TOKENS\n",
    "client = InferenceClient(api_key=api_key)\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are an analyst who answers questions accurately based on coin data and newspaper articles, English question/Korean question.\n",
    "\n",
    "#Previous Chat History:\n",
    "{chat_history}\n",
    "\n",
    "#Question: \n",
    "{question} \n",
    "\n",
    "#Context: \n",
    "{context} \n",
    "\n",
    "#Answer:\"\"\"\n",
    ")\n",
    "\n",
    "def chatbot_conversation():\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in ['exit', 'quit', 'bye']:\n",
    "            print(\"Bye!\")\n",
    "            break\n",
    "        \n",
    "        chat_history = \"\\n\".join([msg['content'] for msg in messages])\n",
    "        context = \"Context information here\"  \n",
    "        \n",
    "        prompt_input = prompt.format(chat_history=chat_history, question=user_input, context=context)\n",
    "        \n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt_input}],\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        bot_reply = completion.choices[0].message['content']\n",
    "        print(f\"Bot: {bot_reply}\")\n",
    "        \n",
    "        messages.append({\"role\": \"assistant\", \"content\": bot_reply})\n",
    "\n",
    "chatbot_conversation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
